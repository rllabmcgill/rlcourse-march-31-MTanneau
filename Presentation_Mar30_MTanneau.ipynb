{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected behaviour of TD(0) in the off-policy prediction\n",
    "\n",
    "## Aka: what makes the deadly triad so deadly\n",
    "\n",
    "Hint: it's all about linear algebra :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fsolve\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Off-policy TD(0) with Linear VFA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Using State Value functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the off-policy case, extended to linear function approximation.\n",
    "\n",
    "Recall the semi-gradient TD update :\n",
    "\\begin{align*}\n",
    "\\delta_{t} &= R_{t+1} + \\gamma \\hat{v}(S_{t+1},\\theta_{t}) - \\hat{v}(S_{t},\\theta_{t}) \\\\\n",
    "\\theta_{t+1} &= \\theta_{t} + \\alpha \\rho_{t} \\delta_{t} \\nabla\\hat{v}(S_{t},\\theta_{t})\n",
    "\\end{align*}\n",
    "\n",
    "The expected behaviour then writes:\n",
    "\\begin{align*}\n",
    "E[\\theta_{t+1}|\\theta_{t}] &= \\theta_{t} + \\alpha E[\\rho_{t} \\delta_{t} \\nabla\\hat{v}(S_{t},\\theta_{t}) | \\theta_{t}]\\\\\n",
    "&= \\theta_{t} + \\alpha E\\left[\\rho_{t} \\big( R_{t+1}\\phi_{t} - \\phi_{t} (\\phi_{t} - \\gamma \\phi_{t+1} )^{T} \\theta_{t} \\big) | \\theta_{t} \\right]\\\\\n",
    "&=\\theta_{t} + \\alpha E\\left[\\rho_{t} R_{t+1}\\phi_{t}\\right] - \\alpha E\\left[\\rho_{t} \\phi_{t} (\\phi_{t} - \\gamma \\phi_{t+1} )^{T}  \\right] \\theta_{t}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now, defining $A,b$ as follow:\n",
    "\\begin{align*}\n",
    "A&=E\\left[\\rho_{t} \\phi_{t} (\\phi_{t} - \\gamma \\phi_{t+1} )^{T}  \\right]\\\\\n",
    "b&=E\\left[\\rho_{t} R_{t+1}\\phi_{t}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "If we expand these expectations, we can write them under the following matrix form. Actually, these equations are at the beginning of the 2015 paper on emphatic TD (off-policy section).\n",
    "\\begin{align*}\n",
    "A&=\\Phi^{T}D_{\\mu}(I-\\gamma P_{\\pi})\\Phi\\\\\n",
    "b&=\\Phi^{T}D_{\\mu}r_{\\pi}\n",
    "\\end{align*}\n",
    "\n",
    "Let's sum up what we know so far :\n",
    "\n",
    "* On-policy. $TD(0)$ converges to the TD fixed point, all is fine\n",
    "\\begin{align*}\n",
    "A&=\\Phi^{T}D_{\\pi}(I-\\gamma P_{\\pi})\\Phi\\\\\n",
    "b&=\\Phi^{T}D_{\\pi}r_{\\pi}\n",
    "\\end{align*}\n",
    "\n",
    "* Off policy, no IS. Not having any IS means we're actually learning the value function of behaviour policy, not that of the target.\n",
    "\\begin{align*}\n",
    "A&=\\Phi^{T}D_{\\mu}(I-\\gamma P_{\\mu})\\Phi\\\\\n",
    "b&=\\Phi^{T}D_{\\mu}r_{\\mu}\n",
    "\\end{align*}\n",
    "\n",
    "* Off-policy, IS. Introducing IS actually corrects part of the equation, but not all of it. In expectation, everything goes as if the transitions occured according to the target policy, but states were sampled according to another distribution (that of the behaviour policy). This corresponds to the case considered in section IX of [Tsitsiklis, Van Roy 1997], where they show that TD can diverge if states are not sampled according to the right distribution.\n",
    "\\begin{align*}\n",
    "A&=\\Phi^{T}D_{\\mu}(I-\\gamma P_{\\pi})\\Phi\\\\\n",
    "b&=\\Phi^{T}D_{\\mu}r_{\\pi}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "In the on-policy case, the convergence prrof relied on the fact that states are sampled from the stationnary distribution $d_{\\pi}$ (see 9.4 in Sutton's book). It is no longer the case in an off-policy. As a consequence, $A$ is no longer guaranteed to be positive definite, and TD may diverge.\n",
    "\n",
    "Indeeed, assuming $A\\theta_{TD} = b$, recall the error behaves as $\\theta_{t+1} - \\theta_{TD} = \\epsilon_{t+1} = (I-\\alpha A) \\epsilon_{t}$. Thus, if $A$ has at least one negative eigenvalue, then TD is bound to diverge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 An example\n",
    "Let's illustrate this in Baird's famous counterexample (picture taken from Sutton's book):\n",
    "<img src=\"baird_scheme.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column sums of key matrix\n",
      "[ 0.14285714  0.14285714  0.14285714  0.14285714  0.14285714  0.14285714\n",
      " -0.84714286]\n",
      "\n",
      "Eigenvalues of A\n",
      "[  5.71428571e-01  -2.39250464e-01  -2.21781072e-02  -7.94718827e-17\n",
      "   5.71428571e-01   5.71428571e-01   5.71428571e-01   5.71428571e-01]\n"
     ]
    }
   ],
   "source": [
    "#number of states\n",
    "n=7\n",
    "\n",
    "#discount factor\n",
    "gamma=0.99\n",
    "r_pi=np.zeros(n)\n",
    "\n",
    "#define stationnary distribution under mu\n",
    "mu=1.0/n * np.ones(n)\n",
    "D_mu =np.diag(mu)\n",
    "#define stationnary distribution under pi\n",
    "pi=np.zeros(n)\n",
    "pi[n-1]=1.0\n",
    "D_pi=np.diag(pi)\n",
    "\n",
    "#Compute transition probability matrices for target and behaviour policy\n",
    "P_pi=np.zeros((7,7))\n",
    "P_pi[:,6]=1\n",
    "\n",
    "P_mu=np.zeros((7,7))\n",
    "P_mu[:,:]=1.0/7.0\n",
    "\n",
    "\n",
    "#Define features matrix\n",
    "Phi = np.zeros((7,8))\n",
    "for i in range(6):\n",
    "    Phi[i,i]=2\n",
    "    Phi[i,7]=1\n",
    "Phi[6,6]=1\n",
    "Phi[6,7]=2\n",
    "\n",
    "\n",
    "I=np.eye(7)\n",
    "\n",
    "#Compute matrix A\n",
    "A=Phi.T.dot(D_mu).dot(I-gamma*P_pi).dot(Phi)\n",
    "\n",
    "#by computing the column sums of the key matrix, we see whether it is positive definite\n",
    "print 'Column sums of key matrix'\n",
    "print np.ones(n).T.dot(D_mu).dot(I-gamma*P_pi)\n",
    "\n",
    "#We TD is bound to diverge (unless the initial theta is really-well chosen) \n",
    "#if A has at least one negative eigenvalue\n",
    "[eA,vA]=np.linalg.eig(A)\n",
    "print '\\nEigenvalues of A'\n",
    "print eA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Are we doomed, though ?\n",
    "\n",
    "Not necessarily, as shows the last line of the proof (still 9.4 in Sutton's book). We need the column sum of the key matrix to be positive. This ought to be the case, assuming $\\mu$ is close enough to $\\pi$.\n",
    "\n",
    "The column sums of the key matrix write as :\n",
    "\\begin{align*}\n",
    "1^{T} D_{\\mu} (I-\\gamma P_{\\pi}) &= d_{\\mu}^{T} (I-\\gamma P_{\\pi})\\\\\n",
    "&= d_{\\mu}^{T} - \\gamma d_{\\mu}^{T} P_{\\pi}\n",
    "\\end{align*}\n",
    "\n",
    "We now assume that, whether we consider the behaviour policy or the target policy, the induced markov chain (with transition matrix $P_{\\pi}$ or $P_{\\mu}$) is irreductible (otherwise, we may restrict to a smaller set of states) and ergodic (otherwise, ie all states are visited infinitely many times when time goes to infinity). Note this assumption is always met when we have exploring starts.\n",
    "\n",
    "Therefore, the associated stationary distribution is unique.\n",
    "\n",
    "As $\\mu \\rightarrow \\pi$, we have $P_{\\pi} \\rightarrow P_{\\mu}$. We also know that $d_{\\pi}$ (resp $d_{\\mu}$) is a left-eigenvector of $P_{\\pi}$ (resp $P_{\\mu}$) associated to left-eigenvalue $1$. We just have to show that $d_{\\mu}$ is close to $d_{\\pi}$ when $\\mu$ is close to $\\pi$\n",
    "\n",
    "Let $\\mu_{k}$ be a sequence of policies that converge to $\\pi$. For simplicity, denote $d_{k}=d_{\\mu_{k}}$, $P_{k}=P_{\\mu_{k}}$ and $\\epsilon_{k}=d_{k}-d_{\\pi}$. We have :\n",
    "\n",
    "\\begin{align*}\n",
    "\\epsilon_{k}^{T} &= d_{k}^{T}-d_{\\pi}^{T}\\\\\n",
    " &= d_{k}^{T}P_{k}-d_{\\pi}^{T} P_{\\pi}\\\\\n",
    " &= (d_{k}-d_{\\pi})^{T} P_{k} + d_{\\pi}^{T} (P_{k}-P_{\\pi})\n",
    "\\end{align*}\n",
    "\n",
    "The sequence $\\epsilon_{k}$ is bounded, so we can extract a convergent subsequence. For simplicity, we assume this is already the case, ie :\n",
    "$$\n",
    "\\epsilon := \\lim_{k \\rightarrow +\\infty} \\epsilon_{k} \n",
    "$$\n",
    "which we re-write with respect to $d_{k}$ :\n",
    "$$\n",
    "d := \\lim_{k \\rightarrow +\\infty} d_{k}\n",
    "$$\n",
    "\n",
    "As $k$ goes to infinity, we have :\n",
    "$$\n",
    "(d-d_{\\pi})^{T} (I-P_{\\pi}) = 0\n",
    "$$\n",
    "\n",
    "And therefore, since $d_{\\pi}^{T}=d_{\\pi}^{T} P_{\\pi}$ :\n",
    "$$\n",
    "d_{\\pi} = d = \\lim_{\\mu \\rightarrow \\pi} d_{\\mu}\n",
    "$$\n",
    "\n",
    "Finally, when $\\mu$ is close to $\\pi$, $d_{\\mu}^{T} - \\gamma d_{\\mu}^{T} P_{\\pi}$ is close to $d_{\\pi}^{T} - \\gamma d_{\\pi}^{T} P_{\\pi}$ which coefficients are all positive. It immediately follows that the key matrix is positive definite if the behaviour policy is close enough to the target policy.\n",
    "\n",
    "##### Conclusion\n",
    "Just like there always exist a behaviour policy that will make off-policy TD with linear approxiamtion diverge, there always exists at least one behaviour policy for which ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Why is TD guaranted to converge in the tabular case ?\n",
    "\n",
    "I believe the answer is in the projector operator $\\Pi$. Recall that :\n",
    "$$\n",
    "\\Pi = \\Phi (\\Phi^{T} D \\Phi)^{-1} \\Phi^{T} D\n",
    "$$\n",
    "\n",
    "The problem with off-policy is that, if $D$ is not the stationnary distribution of the Markov Chain, then the TD operator may not be a contraction with respect to $D$ (see [Tsitsiklis 97]). However, in the tabular case, $\\Phi$ is the identity matrix, which yields immediately $\\Pi=I$, no matter which distribution we consider. As a consequence, $\\Pi \\cdot TD$ is a contraction (because TD is), regardless of the behaviour policy (we still require coverage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Using State-Action functions (aka, Q-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider state-action values. The corresponding algorithm is Expected Sarsa, aka Q-learning when the policy is greedy. Recall the semi-gradient algorithm :\n",
    "\\begin{align*}\n",
    "\\delta_{t} &= R_{t+1} + \\gamma \\sum_{a} \\pi(a|S_{t+1}) \\hat{q}(S_{t+1},a,\\theta_{t}) - \\hat{q}(S_{t},a,\\theta_{t}) \\\\\n",
    "\\theta_{t+1} &= \\theta_{t} + \\alpha \\delta_{t} \\nabla\\hat{q}(S_{t},A_{t},\\theta_{t})\n",
    "\\end{align*}\n",
    "\n",
    "We use linear function approximation, ie : $\\hat{q}(s,a) = \\theta^{T}\\phi(s,a)$. We then have :\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} +\\alpha \\left[ R_{t+1} \\Phi(S_{t},A_{t}) - \\Phi(S_{t},A_{t}) \\big( \\Phi(S_{t},A_{t}) - \\gamma \\sum_{a} \\pi(a|S_{t+1}) \\Phi(S_{t+1},a)\\big)^{T} \\theta_{t} \\right]\n",
    "$$\n",
    "\n",
    "In expectation, updates thus write as (to ease the reading, I voluntarily skip a few lines of calculus here):\n",
    "\\begin{align*}\n",
    "E[\\theta_{t+1}|\\theta_{t}] &= \\theta_{t} +\\alpha E\\left[ R_{t+1} \\Phi(S_{t},A_{t})\\right] - E\\left[ \\Phi(S_{t},A_{t}) \\big( \\Phi(S_{t},A_{t}) - \\gamma \\sum_{a} \\pi(a|S_{t+1}) \\Phi(S_{t+1},a)\\big)^{T}  \\right] \\theta_{t}\\\\\n",
    "&=\\theta_{t} + \\alpha(b-A \\theta_{t})\n",
    "\\end{align*}\n",
    "\n",
    "where (all matrices and vectors are now indexed by state-action pairs) :\n",
    "\\begin{align*}\n",
    "A&=\\Phi^{T}D_{\\mu}(I-\\gamma P_{\\pi})\\Phi\\\\\n",
    "b&=\\Phi^{T}D_{\\mu}r\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The main result here, is that these expression are obtained without using importance sampling. This is due to the fact that Q-learning samples directly from state-action together. Therefore, the transition to the next state $S_{t+1}$ does not depend on the behaviour policy. Besides, the expectation in the target is taken explicitely with respect to $\\pi$. Finally, notice the reward vector $r$ also does not depend on the behaviour policy (nor the target).\n",
    "\n",
    "We have the exact same structure as we had before. Ergo, the conclusions are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What's next ?\n",
    "\n",
    "We've seen that introducing importance sampling partially solves the problem of off-policy. However, it does not correct for the distribution of states. Therefore, we would like to introduce a second set of importance ratios, $\\sigma(s)=\\dfrac{d_{\\pi}(s)}{d_{\\mu}(s)}$, which depend only on the states (not the actions). \n",
    "\n",
    "The update rule would then be, if we use state-value function :\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} + \\alpha \\rho_{t} \\sigma_{t} \\delta_{t} \\nabla\\hat{v}(S_{t},\\theta_{t})\n",
    "$$\n",
    "And for state action values :\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} + \\alpha \\sigma_{t} \\delta_{t} \\nabla\\hat{q}(S_{t},A_{t},\\theta_{t})\n",
    "$$\n",
    "where $\\sigma_{t} = \\dfrac{d_{\\pi}(S_{t})}{d_{\\mu}(S_{t})}$\n",
    "\n",
    "Of course, we have no way of computing neither $d_{\\pi}(s)$ nor $d_{\\mu}(s)$ beforehand...\n",
    "\n",
    "One way to go is developped in [Precup, Sutton, and Dasgupta 2001] (http://www.cs.mcgill.ca/~dprecup/publications/PSD-01.pdf). However, this approach is based on multiplying the importance ratios over an episode, which leads to very high variance.\n",
    "\n",
    "State weighting as is done in Emphatic TD might be another way to tackle the issue."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
